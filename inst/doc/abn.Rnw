% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
\documentclass[nojss]{jss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls - journal of statistical software
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Fraser I. Lewis}
\title{Exploring data from complex systems using Additive Bayesian Networks in \proglang{R}}
%\VignetteIndexEntry{A vignette for the abn package}
%% for pretty printing and a nice hypersummary also set:
%% \Plainauthor{, Second Author} %% comma-separated
\Plaintitle{Multidimensional Bayesian regression in R}
\Shorttitle{The \pkg{abn} package}

\Abstract{
  
  This vignette describes the \pkg{abn} package of \proglang{R} which comprises of model fitting and selection functionality for exploring multivariate data using additive Bayesian network models. These are directed acyclic graphs where each node in the graph comprises a generalized linear model, where a greedy search heuristic is used to identify high scoring models that attempt to identify relationships between all variables in the data. Currently implemented are models for data comprising of categorical and/or continuousqq variables where a logit link is used with the former. Laplace approximations are used to estimate marginal likelihoods and compute marginal posterior densities.       
}

\Keywords{\proglang{R}, Bayesian Networks, additive models, structure discovery}
\Plainkeywords{Bayesian Networks, additive models, structure discovery}

\Address{
  F. I. Lewis\\
  Applied Statistician\\
  Vetsuisse Faculty, University of Zurich\\
  Winterthurerstrasse 270, Zurich 8057\\
  Switzerland\\
  E-mail: \email{fraseriain.lewis@uzh.ch}
}

%% need no \usepackage{Sweave.sty}
%%\SweaveOpts{echo=FALSE,keep.source=TRUE}
\SweaveOpts{keep.source=TRUE,echo=FALSE,eps=FALSE,pdf=TRUE,width=11.69,height=8.27}
\newcommand{\deep}{\setlength{\jot}{12pt}}
\newcommand{\nodeep}{\setlength{\jot}{3pt}}
\begin{document}
\setkeys{Gin}{width=1.0\textwidth}

\section{Introduction}
Bayesian network (BN) modeling (\citeauthor{Buntine1991} \citeyear{Buntine1991}; \citeauthor{HECKERMAN1995} \citeyear{HECKERMAN1995};  \citeauthor{Lauritzen1996} \citeyear{Lauritzen1996}; \citeauthor{Jensen2001} \citeyear{Jensen2001}) is a form of graphical modeling which attempts to separate out indirect from direct association in complex multivariate data, a process typically referred to as structure discovery (\citeauthor{Friedman2003} \citeyear{Friedman2003}). Unlike other widely used multivariate approaches where dimensionality is reduced through exploiting linear combinations of random variables, such as in principal component analysis, graphical modeling does not involve any such dimension reduction. Bayesian networks have been developed for analyzing multinomial, multivariate Gaussian or conditionally Gaussian networks (a mix categorical and Gaussian variables). A number of libraries for fitting such BNs are available from CRAN. These types of BN have been constructed to ensure conjugacy, that is, enable posterior distributions for the model parameters and marginal likelihood to be calculated analytically. The purpose of \pkg{abn} is to provide a library of functions for more flexible BNs which do not rely on conjugacy, which opens up an extremely rich modeling framework but at some considerable additional computational cost. 

Currently \pkg{abn} includes functionality for fitting non-conjugate BN models which are multi-dimensional analogues of combinations of Binomial (logistic) and Gaussian regression. It is planned to extend this to include Poisson distributions for count data and then more complex distributions for overdispersed data such a beta-binomial and negative binomial models.   

The general objective in BN modeling/structure discovery is to perform a model search on the data to identify a locally optimal model. Recall that BN models have a vast search space - super-exponential in the number of nodes - and it is generally impossible to determine a globally optimal model. How best to summarize a set of locally optimal networks with different structural features is an open question, and there are a number of widely used and intuitively reasonable possibilities. For example, one option is to conduct a series of heuristic searches and then simply select the best model found (\citeauthor{HECKERMAN1995} \citeyear{HECKERMAN1995}); alternatively, a single summary network can be constructed using results across many different searches (\citeauthor{Hodges2010} \citeyear{Hodges2010}; \citeauthor{Poon2007} \citeyear{Poon2007}). There are obvious pros and cons to either approach and both are common in the literature and provide a good first exploration of the data. For a general non-technical review of BN modeling applied in biology see \citeauthor{Needham2007} \citeyear{Needham2007}. A case study in applying BN models to epidemiological data using the conjugate BN functionality in \pkg{abn} can be found in \citeauthor{Lewis2011b} \citeyear{Lewis2011b}.

In this vignette we consider a series of examples illustrating how to fit different types of models and run different searches and summary analyzes to a (synthetic) data set comprising of 250 observations from a joint distribution comprising of 17 categorical and 16 continuous variables which is included as part of the library. This data set is a single realization from a network of the same structure as that presented in \citeauthor{Lewis2011b} \citeyear{Lewis2011b}, which is sufficiently complex to provide a realistic example of data mining using Bayesian Network modeling. 
 
\section{Case Study Data}
Figure \ref{fig1}
\begin{figure}[htb]
\includegraphics{var33_MASTER}
\vspace{-1.0cm}
\caption{Directed acyclic graph representation of the joint probability distribution which generated data set {\tt var33} which is included with {\tt abn}. The square nodes are categorical (binary) and the oval nodes continuous variables.} \label{fig1}
\end{figure}
shows the structure of the distribution which generated the data set {\tt var33} included with {\tt abn}. This diagram was created using the {\tt tographviz()} function of {\tt abn} (see later examples) which translates a matrix which defines a network - a directed acyclic graph - into a text file of suitable format for processing in Graphviz, where this processing was done outside of {\tt R}. Graphviz is freely available and operates on most platforms and can be downloaded from {\tt www.graphviz.org}, there is also an R package which interfaces to Graphviz available from the Bioconductor project (requires an installation of Graphviz). 

\section{Fitting a single BN model to data}
In the next four sections we illustrate how to fit a BN model to different kinds of data. The main purpose of BN analyses is to estimate the joint dependency structure of the random variables in the available data, and this is achieved by heuristically searching for optimal models and comparing their goodness of fit using the (log) marginal likelihood, typically referred to as the network score. 

\subsection{Fitting a BN model to categorical data} \label{sec1}
A conjugate Bayesian network applied to categorical data is the classical application of Bayesian network analysis. Here the data are considered as a contingency table of frequency counts and the model describes conditional dependencies between different cells. Note these are not additive models. 

The function {\tt fitbn(data.df, dag.m, prior.obs.per.node=NULL, useK2=FALSE, ...)} fits a multinomial conjugate BN model to the data in {\tt data.df} where the model structure is defined in matrix {\tt dag.m}. There are two choices of priors/goodness of fit metrics; the BDe metric and the K2 metric (see \citeauthor{HECKERMAN1995} \citeyear{HECKERMAN1995}). In brief, in the BDeu metric it is assumed that a number, {\tt prior.obs.per.node}, of prior observations have been observed at each node and these are uniformly distributed across all the hyperparameters at each node. For example in Figure \ref{fig1}, node 4 is conditionally dependent upon node 3, these are binary nodes and the parameter to be estimated is $P(v4=T|v3=T)$ where this has a Beta distributed prior of $Beta(\alpha_1,\alpha_2)$ and supposing {\tt prior.obs.per.node}=16, then with the BDeu metric we have a prior of $Beta(8,8)$. Similarly, if there were two parents for node 4 then there would be four parameters to estimate (assuming both parents were binary) and in this case the prior for each parameter would be $Beta(2,2)$ where again the sum of the hyperparameters equals 16. In contrast, in the K2 metric each and every parameter has a flat prior of $Beta(1,1)$ for binary nodes and Dirichlet $Dir(1,\dots,1)$ for multinomial nodes. An advantage of the BDeu metric is that it is likelihood equivalent and so DAGs which are probabilistically equivalent will have identical BDeu network scores. The K2 metric, however, uses identical uninformative priors for each and every parameter which may also be desirable, but in which case the network scores for probabilistic identical networks may differ (although in practice such differences may be small). In {\tt fitbn}, if the {\tt useK2} argument is {\tt TRUE} then {\tt prior.obs.per.node} is ignored. 

The following code fits a network to the subset of the variables from {\tt var33} which are categorical. In this data these are all binary but {\tt fitbn} works analogously for multinomial variables. Note that all categorical variables should be set as factors - and will be coerced if necessary.     
<<echo=TRUE,print=FALSE>>=
library(abn);# load library
bin.nodes<-c(1,3,4,6,9,10,11,12,15,18,19,20,21,26,27,28,32); 
var33.cat<-var33[,bin.nodes];#categorical nodes only
mydag<-matrix(c(
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v1
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v3
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v4  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v6  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v9  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v10  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v11  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v12  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v15  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v18 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v19
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v20 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v21 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v26 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v27 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v28 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0  #v32 
              ),byrow=TRUE,ncol=17); 
colnames(mydag)<-rownames(mydag)<-names(var33.cat);#set names
## now fit the model defined in mydag - full independence model
fitbn (data.df=var33.cat, dag.m=mydag,useK2=TRUE);
# this is the network score goodness of fit = log marginal likelihood
@
The structure of the network definition matrix is where each row is a ``child'' and each column is its ``parents'', where a {\tt 1} denotes a parent (or arc) is present. Now lets fit a model with some conditional dependencies, for example where {\tt v11} is conditionally dependent upon {\tt v12} and {\tt v10}, and {\tt v4} is conditionally dependent upn {\tt v3}.
<<echo=TRUE,print=FALSE>>=
# now fit model with some conditional dependencies let v11 
## depend jointly on v12 and v10
mydag["v11","v12"]<-1;
mydag["v11","v10"]<-1;
## let v4 depend on v3
mydag["v4","v3"]<-1;
fitbn (data.df=var33.cat, dag.m=mydag,useK2=TRUE);
# network score for a model with conditional independence
@
The network score is considerably improved and therefore suggests support for these new structural features. To produce a visual description of the model then we can export to graphviz as follows
<<echo=TRUE,print=FALSE>>=
tographviz(dag=mydag,data.df=var33.cat,outfile="mydag.dot");#create file
# mydag.dot can then be processed with graphviz
# unix shell "dot -Tpdf mydag.dot -o mydag.pdf" or use gedit if on Windows
@

\begin{figure}[htb]
\includegraphics{mydag}
\vspace{-1.0cm}
\caption{Directed acyclic graph {\tt mydag} created using {\tt tographviz()} and Graphviz} \label{fig2}
\end{figure}
In {\tt tographviz()} the {\tt data.df} argument is used to determine whether the variable is a factor or not, where factors are displayed as squares and non-factors as ovals. To use the full range of visual Graphviz options simply use the file created by {\tt tographviz()} as a template and manually edit this in a text editor.

\subsection{Fitting an additive BN model to categorical data} \label{sec2}
An additive BN model for categorical data can be constructed by considering each individual variable as a logistic regression of the other variables in the data, and hence the network model comprises of many combinations of local logistic regressions. The parameters in this model are the additive terms in a usual logistic regression and independent Gaussian priors are assumed for each covariate. The covariates here must all be binary, and so multinomial variables need to be split into separate binary factors (and added to the original data.frame) in order to form the network model - this is analogous to forming the design matrix in a conventional additive model analysis. Similarly, interaction terms can be added by including appropriate additional columns in the data.frame. In these models the log marginal likelihood (network score) is estimated using Laplace approximations at each node. Hyperparameters for the mean and standard deviations in the Gaussian priors can be specified but some care is required if informative priors are needed at different nodes (see manual page for {\tt fitabn}).

To fit an additive model use {\tt fitabn(data.df,dag.m, ...)}. In the following code we fit first the independence model with no arcs and then the same dependence model as above. Turning on {\tt verbose=TRUE} simply gives the individual log marginal likelihoods for each node (n.b. the numbering is that used internally and simply denotes the variables in the data.frame from left to right).
<<echo=TRUE,print=FALSE>>=
## move back to independence model
mydag["v11","v12"]<-0;mydag["v11","v10"]<-0;mydag["v4","v3"]<-0;
fitabn (data.df=var33.cat,dag.m=mydag,verbose=TRUE);
# now fit the model with some conditional dependencies 
mydag["v11","v12"]<-1;mydag["v11","v10"]<-1;mydag["v4","v3"]<-1;
fitabn (data.df=var33.cat, dag.m=mydag,verbose=TRUE);
# network score for a model with conditional independence
@

\subsection{Fitting an additive BN model to continuous data} \label{sec3} 
We now consider analogous models to those in Section \ref{sec2} but where the network comprises of Gaussian linear regressions rather than logistic regressions. The structure of these models again assumes independent Gaussian priors for each of the coefficients in the additive components for the mean response at each node, and as the model is parameterized in terms of precision $(1/\sigma^2)$, independent Gamma priors are used for the precision parameter at each node. 
<<echo=TRUE,print=FALSE>>=
var33.cts<-var33[,-bin.nodes];#drop categorical nodes
mydag<-matrix(c(
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v2
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v5  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v7  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v8 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v13  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v14  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v16  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v17  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v22 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v23
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v24 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v25 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v29 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v30 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v31 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0  #v33 
              ),byrow=TRUE,ncol=16); 
colnames(mydag)<-rownames(mydag)<-names(var33.cts);#set names
## now fit the model defined in mydag - full independence
fitabn (data.df=var33.cts,dag.m=mydag,verbose=TRUE);
## uses default priors of N(mu=0,var=1000), 1/var=Gamma(0.001,1/0.001)
# this is the network score goodness of fit = log marginal likelihood
@
Now fit a model with conditional independencies, for example 
<<echo=TRUE,print=FALSE>>=
# now fit model with some conditional dependencies let v33 
## depend on v31, and v24 depend on 23, and v14 depend on v13
mydag["v33","v31"]<-1;
mydag["v24","v23"]<-1;
mydag["v14","v13"]<-1;
fitabn (data.df=var33.cts, dag.m=mydag,verbose=TRUE);
# network score for a model with conditional independence
tographviz(dag=mydag,data.df=var33.cts,outfile="mydagcts.dot");#create file
# mydag.dot can then be processed with graphviz
# unix shell "dot -Tpdf mydagcts.dot -o mydagcts.pdf" or use gedit if on Windows
@
\begin{figure}[htb]
\includegraphics{mydagcts}
\vspace{-1.0cm}
\caption{Directed acyclic graph {\tt mydag} for continuous variables only created using {\tt tographviz()} and Graphviz} \label{fig3}
\end{figure}


\subsection{Fitting an additive BN model to mixed data} \label{sec4}
To conclude the fitting of a single pre-specified model to data we consider an additive BN model which comprises both binary and Gaussian nodes and this comprises of a combination of Binomial (logistic) and Gaussian linear models. Again {\tt fitabn()} is used and the code is almost identical to the previous examples. 
<<echo=TRUE,print=FALSE>>=
mydag<-matrix(c(
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#1
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#2  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#3  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#4  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#5  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#6  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#7  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#8  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#9  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#10 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#11 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#12 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#13 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#14 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#15 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#16 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#17 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#18 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#19 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#20 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#21 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#22 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#23 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#24 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#25 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#26 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#27 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#28 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#29 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#30
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#31
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#32 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 #33
                                               ),byrow=TRUE,ncol=33); 
colnames(mydag)<-rownames(mydag)<-names(var33);#set names
## now fit the model defined in mydag - full independence
fitabn (data.df=var33,dag.m=mydag,verbose=TRUE);
@
We now fit a BN model which has the same structure as the joint distribution used to generate the data and then create a visual graph of this model
<<echo=TRUE,print=FALSE>>=
# define a model with many independencies
mydag[2,1]<-1;
mydag[4,3]<-1;
mydag[6,4]<-1; mydag[6,7]<-1;
mydag[5,6]<-1;
mydag[7,8]<-1;  
mydag[8,9]<-1;
mydag[9,10]<-1;
mydag[11,10]<-1; mydag[11,12]<-1; mydag[11,19]<-1;
mydag[14,13]<-1;
mydag[17,16]<-1;mydag[17,20]<-1;
mydag[15,14]<-1; mydag[15,21]<-1;
mydag[18,20]<-1;
mydag[19,20]<-1;
mydag[21,20]<-1;
mydag[22,21]<-1;
mydag[23,21]<-1;
mydag[24,23]<-1;
mydag[25,23]<-1; mydag[25,26]<-1;
mydag[26,20]<-1;
mydag[33,31]<-1;
mydag[33,31]<-1;
mydag[32,21]<-1; mydag[32,31]<-1;mydag[32,29]<-1;    
mydag[30,29]<-1;
mydag[28,27]<-1; mydag[28,29]<-1;mydag[28,31]<-1;       
fitabn (data.df=var33, dag.m=mydag);
# network score for a model with conditional independence
tographviz(dag=mydag,data.df=var33,outfile="mydag_all.dot");#create file
# mydag.dot can then be processed with graphviz
# unix shell "dot -Tpdf mydag_all.dot -o mydag_all.pdf" or use gedit if on Windows
@
\begin{figure}[htb]
\includegraphics{mydag_all}
\vspace{-1.0cm}
\caption{Directed acyclic graph {\tt mydag} for mixed continuous and discrete variables} \label{fig4}
\end{figure}

\subsection{Model fitting validation}
In order to validate the conjugate models, network scores, for both overall networks and individual nodes using the Bayesian Dirichlet equivalence uniform (BDeu) metric were compared with the {\tt deal} library available from {\tt CRAN}. This metric can be used by {\tt useK2=FALSE} and providing an explicit value for {\tt prior.obs.per.node}. A wide range of models for multinomial data were compared and these were always identical to those values produced by {\tt deal}. To validate the additive models mixed categorical and continuous models, estimates of the posterior distributions for the model parameters using Laplace approximations (see later) were compared with those estimated using Markov chain Monte Carlo. These were always in very close agreement for the range of models and data examined. This is an indirect validation of the Laplace estimate of the network score, e.g. if the posterior densities match closely then this implies that the denominator (the marginal likelihood - network score) must also be accurately estimated, as a ``gold standard'' estimate of the network score is generally unavailable for such non-conjugate models. 

\section{Searching for Optimal Models} \label{sec5}
A key purpose of BN modeling is to estimate the dependency structure in multivariate data - that is, find a DAG which is robust and representative of the dependency structure of the (unknown) system of processes which generated the observed data. The challenge here is that with such a vast model space it is impossible to enumerate over all possible DAGs, and there may be very many different DAGs with similar goodness of fit. In the next sections we first consider searching for categorical (conjugate) BN models, then additive models. 

\subsection{Single search for optimal BN model for categorical data}
To run a single search heuristic use {\tt searchbn()} which starts from a randomly chosen DAG (created by randomly adding arcs to an empty network {\tt init.permuts} times) and then searches stepwise for an improved structure, where three stepwise operations are possible: i) add an arc; ii) remove and arc; or iii) reverse and arc. The stepwise search is subject to a number of conditions, firstly only moves that do not generate a cycle are permitted, secondly, a parent limit is imposed which fixes the maximum number of parents which each child node can have (arcs go from parent to child), and thirdly it is possible to use ban or retain constraints. If provided, {\tt banned.m} is a matrix which defines arcs that are not allowed to be considered in the search process (or in the creation of the initial random network). Similarly, {\tt retain.m} includes arcs which must always be included in any model, and again this includes the initial random network. It is also possible to specific an explicit starting matrix, {\tt start.m}. Note that only very rudimentary checking is done to make sure that the ban, retain and start networks - if user supplied - are not contradictory.

To improve the computational performance of {\tt searchbn()} by default a node cache is used, this is where rather than re-calculate the score for each individual node in the network (the overall network score is the product of all the scores for the individual nodes) the score for each unique node found during the search is stored in a lookup table. This can make very significant improvements in speed and the default is for a search to terminate prematurely if the node cache is exceeded, this behaviour can be turned off by {\tt enforce.db.size=FALSE}), but it is generally advisable to use a sufficiently large value for {\tt db.size} to avoid this (a warning will appear to say this limit has been reached if {\tt enforce.db.size=FALSE}).

<<echo=TRUE,print=FALSE,eval=FALSE>>=
bin.nodes<-c(1,3,4,6,9,10,11,12,15,18,19,20,21,26,27,28,32); 
var33.cat<-var33[,bin.nodes];#categorical nodes only
mydag<-matrix(c(
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v1
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v3
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v4  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v6  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v9  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v10  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v11  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v12  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v15  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v18 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v19
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v20 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v21 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v26 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v27 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v28 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0  #v32 
              ),byrow=TRUE,ncol=17); 
colnames(mydag)<-rownames(mydag)<-names(var33.cat);#set names
## create empty DAGs
banned.cat<-matrix(rep(0,dim(var33.cat)[2]^2),ncol=dim(var33.cat)[2]);
colnames(banned.cat)<-rownames(banned.cat)<-names(var33.cat);#set names
retain.cat<-matrix(rep(0,dim(var33.cat)[2]^2),ncol=dim(var33.cat)[2]);
colnames(retain.cat)<-rownames(retain.cat)<-names(var33.cat);#set names
start.cat<-matrix(rep(0,dim(var33.cat)[2]^2),ncol=dim(var33.cat)[2]);
colnames(start.cat)<-rownames(start.cat)<-names(var33.cat);#set names
myres<-searchbn(data.df=var33.cat,
                banned.m=banned.cat,
                retain.m=retain.cat,
                start.m=start.cat,
                useK2=TRUE,max.parents=2,init.permuts=0,db.size=1000);
@

\subsection{Single search for optimal additive BN model for categorical data}
To run a single search heuristic for an additive BN use {\tt searchabn()} which is very similar to {\tt searchbn()}, the main difference is in the parameter prior specifications. It only makes sense to use uninformative priors for the parameters for each variable as these are fixed for all DAG structures e.g. parameter priors are not structure specific. By default diffuse priors of Gaussians of mean zero and precision of 1000 are used, where these parameters are for the usual additive terms in a logistic regression. These can be overridden in the command line arguments if desired (not advisable). Several additional arguments are available which relate to the numerical routines used in the Laplace approximation to calculate the network score. The defaults appear to work reasonably well in practice and if it is not possible to calculate a robust value for this approximation in any model, for example due to a singular design matrix at one or more nodes, then this model is simply assigned a log network score of $-\infty$ which effectively removes it from the model search.  

<<echo=TRUE,print=FALSE,eval=FALSE>>=
## just use default priors 
myres.add<-searchabn(data.df=var33.cat,
                 banned.m=banned.cat,
                 retain.m=retain.cat,
                 start.m=start.cat,
                 max.parents=2, 
                 init.permuts=0,db.size=1000,error.verbose=TRUE);
@

\subsection{Single search for optimal BN model for continuous data} \label{sec4a}
As above but for a network of Gaussian nodes.
<<echo=TRUE,print=FALSE>>=
var33.cts<-var33[,-bin.nodes];#drop categorical nodes
mydag<-matrix(c(
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v2
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v5  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v7  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v8 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v13  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v14  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v16  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v17  
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v22 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v23
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v24 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v25 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v29 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v30 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #v31 
                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0  #v33 
              ),byrow=TRUE,ncol=16); 
colnames(mydag)<-rownames(mydag)<-names(var33.cts);#set names
banned.cts<-matrix(rep(0,dim(var33.cts)[2]^2),ncol=dim(var33.cts)[2]);
colnames(banned.cts)<-rownames(banned.cts)<-names(var33.cts);#set names
retain.cts<-matrix(rep(0,dim(var33.cts)[2]^2),ncol=dim(var33.cts)[2]);
colnames(retain.cts)<-rownames(retain.cts)<-names(var33.cts);#set names
start.cts<-matrix(rep(0,dim(var33.cts)[2]^2),ncol=dim(var33.cts)[2]);
colnames(start.cts)<-rownames(start.cts)<-names(var33.cts);#set names
#
myres.add<-searchabn(data.df=var33.cts,
                 banned.m=banned.cts,
                 retain.m=retain.cts,
                 start.m=start.cts,
                 max.parents=2, 
                 init.permuts=0,db.size=1000,error.verbose=TRUE);
@

\subsection{Single search for optimal additive BN model for mixed data}
Model searching for mixed data is again very similar to the previous examples. Note that in this example the parameter priors are specified explicitly (although those given are the same as the defaults). The {\tt +1} in the hyperparameter specification is because a constant term is included in the additive formulation for each node.

<<echo=TRUE,print=FALSE,eval=FALSE>>=
mydag<-matrix(c(
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#1
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#2  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#3  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#4  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#5  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#6  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#7  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#8  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#9  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#10 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#11 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#12 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#13 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#14 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#15 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#16 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#17 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#18 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#19 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#20 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#21 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#22 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#23 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#24 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#25 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#26 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#27 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#28 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#29 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#30
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#31
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#32 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 #33
                                               ),byrow=TRUE,ncol=33); 
colnames(mydag)<-rownames(mydag)<-names(var33);#set names
## create empty DAGs
banned<-matrix(rep(0,dim(var33)[2]^2),ncol=dim(var33)[2]);
colnames(banned)<-rownames(banned)<-names(var33);#set names
retain<-matrix(rep(0,dim(var33)[2]^2),ncol=dim(var33)[2]);
colnames(retain)<-rownames(retain)<-names(var33);#set names
start<-matrix(rep(0,dim(var33)[2]^2),ncol=dim(var33)[2]);
colnames(start)<-rownames(start)<-names(var33);#set names

## giving diffuse priors - same as default but explicitly stated
myres.add<-searchabn(data.df=var33,
                 banned.m=banned,
                 retain.m=retain,
                 start.m=start,
                 hyper.params=list(
                                   mean=rep(0,dim(var33)[2]+1),
                                   sd=rep(sqrt(1000),dim(var33)[2]+1), 
                                   shape=rep(0.001,16),## 16 Gaussian nodes
                                   scale=rep(1/0.001,16)## 16 Gaussian nodes
                 ),
                 max.parents=2, 
                 init.permuts=0,db.size=10000,
                 error.verbose=TRUE,enforce.db.size=TRUE);
@

\section{Multiple Search Strategies}
To estimate a robust BN or additive BN for a given dataset is it necessary to run many searches and then summarize the results of these searches. The functions {\tt hillsearchbn()} and {\tt hillsearchabn()} are similar {\tt searchbn()} and {\tt searchabn()} but run multiple searches. There are some differences. Firstly, it is necessary to provide a list of starting networks - these can all be the null network - but must be explicitly given, and must have the same number of entries as the number of searches requested. Secondly, these functions also use a node cache for speed and there is the option now to either use a single joint node cache over all  searches, or else use a local node cache which is reset to empty at the start of each new search. The parameter which governs this is {\tt localdb} and defaults to true which resets the cache at the start of each new search. 

Conceptually it may seem more efficient to use one global node cache to allow node information to be shared between different searches, however, in practice as the search space is so vast for some problems this can result in extremely \emph{slow} searches. As the cache becomes larger it can take much more time to search it (and it may need to be searched a very large number of times) than to simply perform the appropriate numerical computation. Profiling using the google performance tool google-pprof suggests that more than 80\% of the computation time may be taken up by lookups. When starting searches from different random places in the model space the number of individual node structures in common between any two searches, relative to the total number of different node structures searched over can be very small meaning a common node cache is inefficient. This may not be the case when starting networks are relatively similar.   

It is suggested to use {\tt localdb=FALSE} with some caution as it may lead to computations, the duration of which, is hard to estimate as each successive search becomes slower as the node cache increases. To help with performance monitoring it is possible to turn on timings using {\tt timing=TRUE} which then outputs the number of seconds of CPU time each individual search takes (using standard libc functions declared in time.h).

<<echo=TRUE,print=FALSE,eval=FALSE>>=
mydag<-matrix(c(
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#1
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#2  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#3  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#4  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#5  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#6  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#7  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#8  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#9  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#10 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#11 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#12 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#13 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#14 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#15 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#16 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#17 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#18 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#19 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#20 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#21 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#22 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#23 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#24 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#25 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#26 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#27 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#28 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#29 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#30
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#31
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#32 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 #33
                                               ),byrow=TRUE,ncol=33); 
colnames(mydag)<-rownames(mydag)<-names(var33);#set names
## create empty DAGs
banned<-matrix(rep(0,dim(var33)[2]^2),ncol=dim(var33)[2]);
colnames(banned)<-rownames(banned)<-names(var33);#set names
retain<-matrix(rep(0,dim(var33)[2]^2),ncol=dim(var33)[2]);
colnames(retain)<-rownames(retain)<-names(var33);#set names
start<-matrix(rep(0,dim(var33)[2]^2),ncol=dim(var33)[2]);
colnames(start)<-rownames(start)<-names(var33);#set names

set.seed(10000);## only affects init.permuts
start.list<-list();
n.searches<-10;#example only - must be much larger in practice
for(i in 1:n.searches){start.list[[i]]<-retain;} ## empty networks
myres<-hillsearchabn(data.df=var33,banned.m=banned,retain.m=retain,
                     start.m=start.list,
                     hyper.params=list(
                     mean=rep(0,dim(var33)[2]+1),
                     sd=rep(sqrt(1000),dim(var33)[2]+1), 
                     shape=rep(0.001,16),## 16 Gaussian nodes
                     scale=rep(1/0.001,16)## 16 Gaussian nodes
                     ),
                     max.parents=2, 
                     num.searches=n.searches,
                     init.permuts=20,db.size=20000,
                     localdb=TRUE,timing=TRUE);

@
\subsection{Creating a Summary Network - Majority Consensus}
One approach to producing a single robust BN model of the data is to mimic the approach used in phylogenetics to create majority consensus trees. A DAG is constructed comprising of all the arcs present in more than 50\% of the DAGs found from the search heuristics, that is all the locally optimal models found are combined into a single summary network. Combining results from different runs of {\tt searchbn()} or {\tt searchabn()} is straightforward, although note that it is necessary to check for duplicate random starting networks (unlikely generally but not impossible). The following code provides a simple way to produce a majority consensus network and Figure \ref{fig5} shows the resulting network - note that this is an example only and many thousands of searches may need to be conducted to achieve robust results. One simple ad-hoc method for assessing how many searches are needed is to run a number of searches and split the results into two (random) groups, and calculate the majority consensus network within each group. If these are the same then it suggests that sufficient searches have been run.   

\begin{figure}[htb]
\includegraphics{dagcon}
\vspace{-1.0cm}
\caption{Example majority consensus network (from the results of only 10 searches)} \label{fig5}
\end{figure}


<<echo=TRUE,print=FALSE,eval=FALSE>>=
# use results from above searches which are stored in ``myres''
#step 1. discard any duplicate searches (these are unlikely)
indexes<-uniquenets(myres$init.dag);
all.res<-list();
all.res$init.score<-myres$init.score[indexes];
all.res$final.score<-myres$final.score[indexes];
all.res$init.dag.<-myres$init.dag[indexes];
all.res$final.dag<-myres$final.dag[indexes];
# for every possible arc calculate how many times it appears in the searches  
mypruneddags<-prunenets(all.res$final.dag,round(0.51*length(all.res$final.dag)));
# now get a matrix/DAG for the majority network comprising of 1/0s
myfunc<-function(arg1,threshold,netdata){#trivial helper for apply()
         if(arg1>=round(threshold*length(netdata$final.dag)))
         {return(1);} else {return(0);}}
dag.con<-apply(mypruneddags$arcs.sum,c(1,2),FUN=myfunc,threshold=0.51,
               netdata=all.res);
tographviz(dag=dag.con,data.df=var33,outfile="dagcon.dot");#create file
# dagcon.dot can then be processed with graphviz
# unix shell "dot -Tpdf dagcon.dot -o dagcon.pdf" or use gedit if on Windows

@


\section{Larger Scale Problems}
Determining an optimal DAG (e.g. a majority consensus model) for data sets with larger numbers of variables can require additional ad-hoc measures beyond limiting the number number of parents per node. One approach is to iteratively limit the search space by adaptively adjusting the ban and retain lists. Initially set the parent limit to one and conduct a large number of searches, and check that the majority consensus network is robust. Next, for those variables in the majority consensus network which had (strictly) less than the current number of parent limit arcs (e.g. one) append these to the ``ban list'' so that future searches will not again consider adding further arcs to these variables. Then create a ``retain list'' containing all those arcs in the current majority consensus network so that in all subsequent searches those arcs that are currently supported will be retained automatically through all searches (and set the start network to the retain network - plus some random permutations). Now increase the parent limit (e.g. to two) and repeat the whole procedure. This approach successively limits the search space finally narrowing to a summary majority consensus network, although a number of iterations might be needed. Each iteration of this approach can still take many hours on a multi-processor compute server depending on the number of variables in the data.

\section{Estimating Posterior Densities}
After determining an appropriate BN model it is typical to estimate the parameter effects, e.g. 95\% posterior intervals, which is done using the function {\tt getmarginal()} which again uses Laplace approximations. An appropriate domain (range) for each parameter needs to be supplied which can either be done by some trial and error, searching for where the mode of the posterior density is, or else by using the classic {\tt glm()} function in {\tt R} to provide an approximate range. Given below are three examples, and note that this function works with one node and one parameter in that node at a time.  

<<echo=TRUE,print=FALSE>>=
#specific a DAG model
mydag<-matrix(c(
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#1
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#2  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#3  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#4  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#5  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#6  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#7  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#8  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#9  
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#10 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#11 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#12 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#13 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#14 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#15 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#16 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#17 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#18 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#19 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#20 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#21 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#22 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#23 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#24 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#25 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#26 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#27 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#28 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#29 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#30
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#31
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,#32 
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 #33
                                               ),byrow=TRUE,ncol=33); 
colnames(mydag)<-rownames(mydag)<-names(var33);#set names
## now fit the model defined in mydag - full independence
# define a model with many independencies
mydag[2,1]<-1;
mydag[4,3]<-1;
mydag[6,4]<-1; mydag[6,7]<-1;
mydag[5,6]<-1;
mydag[7,8]<-1;  
mydag[8,9]<-1;
mydag[9,10]<-1;
mydag[11,10]<-1; mydag[11,12]<-1; mydag[11,19]<-1;
mydag[14,13]<-1;
mydag[17,16]<-1;mydag[17,20]<-1;
mydag[15,14]<-1; mydag[15,21]<-1;
mydag[18,20]<-1;
mydag[19,20]<-1;
mydag[21,20]<-1;
mydag[22,21]<-1;
mydag[23,21]<-1;
mydag[24,23]<-1;
mydag[25,23]<-1; mydag[25,26]<-1;
mydag[26,20]<-1;
mydag[33,31]<-1;
mydag[33,31]<-1;
mydag[32,21]<-1; mydag[32,31]<-1;mydag[32,29]<-1;    
mydag[30,29]<-1;
mydag[28,27]<-1; mydag[28,29]<-1;mydag[28,31]<-1;
@
<<echo=TRUE,print=FALSE>>=  
#now get the marginal distribution for the parameters of v33     
marg1<-getmarginal(data.df=var33,
                   dag.m=mydag,
                   whichnode="v33",
                   whichvar="constant",#this is the intercept
                    hyper.params=list(
                     mean=rep(0,dim(var33)[2]+1),
                     sd=rep(sqrt(1000),dim(var33)[2]+1), 
                     shape=rep(0.001,16),## 16 Gaussian nodes
                     scale=rep(1/0.001,16)## 16 Gaussian nodes
                     ),
                     post.x=seq(from=-1.5,to=-0.5,len=1000),
                     verbose=TRUE);
cum.marg<-cumsum(marg1[,"f"])/sum(marg1[,"f"])
marg<-cbind(marg1,cum.marg);
marg[which(marg[,3]>0.5)[1]];#approx.median
@
<<echo=TRUE,print=FALSE>>=  
## now for the precision at node v33
marg2<-getmarginal(data.df=var33,
                   dag.m=mydag,
                   whichnode="v33",
                   whichvar="precision",#this is the intercept
                    hyper.params=list(
                     mean=rep(0,dim(var33)[2]+1),
                     sd=rep(sqrt(1000),dim(var33)[2]+1), 
                     shape=rep(0.001,16),## 16 Gaussian nodes
                     scale=rep(1/0.001,16)## 16 Gaussian nodes
                     ),
                     post.x=seq(from=0.5,to=1.5,len=1000),
                     verbose=TRUE);
cum.marg<-cumsum(marg2[,"f"])/sum(marg2[,"f"])
marg<-cbind(marg2,cum.marg);
marg[which(marg[,3]>0.5)[1]];#approx.median
@
<<echo=TRUE,print=FALSE>>= 
## now for a covariate effect at node v6
marg3<-getmarginal(data.df=var33,
                   dag.m=mydag,
                   whichnode="v6",
                   whichvar="v4",#this is the covariate effect
                    hyper.params=list(
                     mean=rep(0,dim(var33)[2]+1),
                     sd=rep(sqrt(1000),dim(var33)[2]+1), 
                     shape=rep(0.001,16),## 16 Gaussian nodes
                     scale=rep(1/0.001,16)## 16 Gaussian nodes
                     ),
                     post.x=seq(from=-1.5,to=2.5,len=1000),
                     verbose=TRUE);
cum.marg<-cumsum(marg3[,"f"])/sum(marg3[,"f"])
marg<-cbind(marg3,cum.marg);
marg[which(marg[,3]>0.5)[1]];#approx.median
@
Figure \ref{fig6}
\begin{figure}[htbp]
<<fig=TRUE,echo=FALSE,keep.source=FALSE>>=
par(mar=c(8.8,8.2,3.1,3.1));
par(cex.axis=2.5);par(cex.lab=2.5);par(bg="white");par(fg="black");par(col.axis="black");par(col="black");par(col.main="black");
par(cex.main=2.5);par(col.lab="black");par(las=1);par(xaxs="i");par(yaxs="i");
par(mfrow=c(1,1));
plot(marg1[,"x"],marg1[,"f"],main="",xlab="",ylab="",type="n",axes=FALSE,xlim=c(-2,3),ylim=c(0,8));
lines(marg1[,"x"],marg1[,"f"],col="brown",lwd=3,lty=1);
lines(marg2[,"x"],marg2[,"f"],col="darkblue",lwd=3,lty=6);
lines(marg3[,"x"],marg3[,"f"],col="darkgreen",lwd=3,lty=4);
mtext("log odds",1,line=4,cex=2.5);
par(las=3);
mtext("Density",2,line=4.5,cex=2.5);par(las=1);
axis(1,padj=0.5,cex.axis=2.0);axis(2);box(); title("");
legend(0.5,8,legend=c("intercept node v33","Precision node v33","effect of v4 at node v6"),cex=1.5,
 col=c("brown","darkblue","darkgreen"),lty=c(1,6,4),lwd=5,bty="n");
@
\vspace{-1.0cm}
\caption{Some Posterior densities} \label{fig6}
\end{figure}
shows an example of posterior densities estimated using {\tt getmarginal()}, all posterior densities for all parameters in the additive BN can be estimated in the same way.
\newpage
\bibliography{abn}


\end{document}
